@book{knuth,
    author = "Donald Ervin Knuth",
    title = "The Art of Computer Programming",
    volume = "1",
    publisher = "Addison-Wesley Professional",
    year = "1997"
}

@misc{difflect,
    author={Salil Vadhan and Alan Deckelbaum and Emily Shen and Thomas Steinke},
    month={2},
    day={9},
    year={2013},
    title={Notes on the Definition of Differential Privacy}
}

@article{diffbook,
  title={The algorithmic foundations of differential privacy.},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Found. Trends Theor. Comput. Sci.},
  volume={9},
  number={3-4},
  pages={211--407},
  year={2014}
}


@inproceedings{deep,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  month = oct,
  series = {{{CCS}} '16},
  pages = {308--318},
  publisher = {{Association for Computing Machinery}},
  address = {{Vienna, Austria}},
  doi = {10.1145/2976749.2978318},
  isbn = {978-1-4503-4139-4}
}


@misc{htraerp,
    title={How to Read an Engineering Research Paper},
    author={William G. Griswold},
}
@article{prune,
  author    = {Hidenori Tanaka and
               Daniel Kunin and
               Daniel L. K. Yamins and
               Surya Ganguli},
  title     = {Pruning neural networks without any data by iteratively conserving
               synaptic flow},
  journal   = {CoRR},
  volume    = {abs/2006.05467},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.05467},
  eprinttype = {arXiv},
  eprint    = {2006.05467},
  timestamp = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-05467.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lth,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.03635},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.03635},
  eprinttype = {arXiv},
  eprint    = {1803.03635},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-03635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{pegrad,
      title={Efficient Per-Example Gradient Computations},
      author={Ian Goodfellow},
      year={2015},
      print={1510.01799},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{montesys,
title = {Monte Carlo and Las Vegas Randomized Algorithms for Systems and Control*: An Introduction},
journal = {European Journal of Control},
volume = {13},
number = {2},
pages = {189-203},
year = {2007},
issn = {0947-3580},
doi = {https://doi.org/10.3166/ejc.13.189-203},
url = {https://www.sciencedirect.com/science/article/pii/S0947358007708191},
author = {Roberto Tempo and Hideaki Ishii},
keywords = {Systems and control, Probabilistic approach, Randomized algorithms, Monte Carlo algorithms, Las Vegas algorithms},
abstract = {In this paper, we present an introduction to Monte Carlo and Las Vegas randomized algorithms for systems and control. Specific applications of these algorithms include stability analysis, Lyapunov functions, and distributed consensus problems.}
}

@inproceedings{approx-k-anon,
       booktitle = {Proceedings of the International Conference on Database Theory (ICDT 2005)},
           month = {November},
           title = {Approximation Algorithms for k-Anonymity},
          author = {Gagan Aggarwal and Tomas Feder and Krishnaram Kenthapadi and Rajeev Motwani and Rina Panigrahy and Dilys Thomas and An Zhu},
            year = {2005},
         journal = {Journal of Privacy Technology (JOPT)},
             url = {http://ilpubs.stanford.edu:8090/645/},
        abstract = {We consider the problem of releasing a table containing personal records, while ensuring individual privacy and maintaining data integrity to the extent possible. One of the techniques proposed in the literature is k-anonymization. A release is considered k-anonymous if the information corresponding to any individual in the release cannot be distinguished from that of at least k-1 other individuals whose information also appears in the release. In order to achieve k-anonymization, some of the entries of the table are either suppressed or generalized (e.g. an Age value of 23 could be changed to the Age range 20-25). The goal is to lose as little information as possible while ensuring that the release is k-anonymous. This optimization problem is referred to as the k-Anonymity problem. We show that the k-Anonymity problem is NP-hard even when the attribute values are ternary and we are allowed only to suppress entries. On the positive side, we provide an O(k)-approximation algorithm for the problem. We also give improved positive results for the interesting cases with specific values of k --- in particular, we give a 1.5-approximation algorithm for the special case of 2-Anonymity, and a 2-approximation algorithm for 3-Anonymity. }
}

@misc{scalable-opt,
      title={Scalable Adaptive Stochastic Optimization Using Random Projections}, 
      author={Gabriel Krummenacher and Brian McWilliams and Yannic Kilcher and Joachim M. Buhmann and Nicolai Meinshausen},
      year={2016},
      eprint={1611.06652},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{clipping,
  title={Understanding gradient clipping in incremental gradient methods},
  author={Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1504--1512},
  year={2021},
  organization={PMLR}
}


@article{semidefinite,
author = {Goemans, Michel X. and Williamson, David P.},
title = {Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/227683.227684},
doi = {10.1145/227683.227684},
journal = {J. ACM},
month = {nov},
pages = {1115â€“1145},
numpages = {31},
keywords = {randomized algorithms, satisfiability, Approximation algorithms, convex optimization}
}

@book{blyth,
  title={An Introduction to Quantitative Finance},
  author={Blyth, S.},
  isbn={9780199666591},
  lccn={2013941968},
  url={https://books.google.com/books?id=SXbcAAAAQBAJ},
  year={2013},
  publisher={OUP Oxford}
}

@article{skew-orthogonal-convolutions,
  author    = {Sahil Singla and
               Soheil Feizi},
  title     = {Skew Orthogonal Convolutions},
  journal   = {CoRR},
  volume    = {abs/2105.11417},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.11417},
  eprinttype = {arXiv},
  eprint    = {2105.11417},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-11417.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{stabilizing-gradients,
  title = 	 {Stabilizing Gradients for Deep Neural Networks via Efficient {SVD} Parameterization},
  author =       {Zhang, Jiong and Lei, Qi and Dhillon, Inderjit},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5806--5814},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/zhang18g/zhang18g.pdf},
  url = 	 {https://proceedings.mlr.press/v80/zhang18g.html},
  abstract = 	 {Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed Spectral-RNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially in capturing long range dependencies, as shown on the synthetic addition and copy tasks, as well as on MNIST and Penn Tree Bank data sets.}
}
