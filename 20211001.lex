I was in my genetic algorithms class yesterday contemplating term projects, and
I devised the following research questions:

\begin{research}
    Can genetic programming be used in finding solutions to ordinary/partial
    differential equations?
\end{research}

\begin{research}
    Is there an optimal set of weights to be used in direct feedback alignment?
    Can we find these using genetic algorithms?
\end{research}

\begin{research}
    What is the optimal structure of a neural network given a problem
    description? Can this be found using genetic algorithms? 
\end{research}

I haven't looked too far into the literature on these topics, but these would be
interesting problems to investigate.

I'm also filling in the backlog of questions I had written down in my research
journal:

\begin{research}
    Is the collatz conjecture Turing Complete? More abstractly, what does it
    mean for an equation to be Turing Complete?
\end{research}

\begin{research}
    Is there a continuous analgous to most algorithms? In a sense, algorithms
    can be structured as finite state machines where step $i$ goes directly to
    step $i + 1$ or jumps to some step $i + j$. Is there a way to consider the
    operations which occur between step $i$ and step $i + 1$, i.e. some step $i
    + \epsilon$ where $\epsilon \in (0, 1)$? This question may be phrased weird,
    but as an example, what is the continuous analog of a bubble sort algorithm?
    How can this be modeled as a dynamic change in systems rather than the
    discretized swapping of elements in an array?
\end{research}
