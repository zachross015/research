\section{\sectioncite{federated-learning}}
\label{sec:federated-learning}

This paper outlines a distributed learning method which operates on the basis
that most the devices involved in the learning process are heterogenous and
have unreliable internet connectivity. The synchronized federated learning
algorithm generally follows these steps:
\begin{enumerate}
    \item a subset of existing clients is selected, each of which downloads the
        current model;
    \item each client in the subset computes an updated model based on their
        local data;
    \item the model updates are sent from the selected clients to the server;
        and finally
    \item the server aggregates these models (typically by averaging) to
        construct an improved global model.
\end{enumerate}
The main contribution deals with deriving a method of federated learning which
doesn't require full model updates being sent back and forth between the client
and server, for which they derive two primary approaches:
\begin{description}
    \item[Structured updates] Updates are directly learned from a restricted
        space that can be parameterized using a smaller number of variables.
    \item[Sketched updates] A full model is learned and then compressed prior to
        sending it to the server.
\end{description}
The general scheme is to learn a real matrix $\mat{W} \in \R^{d_1 \times d_2}$
from data stored across a large number of clients. This is done by sending a
current model $\mat{W}_t$ to a subset $S_t$ of $n_t$ clients. Each client
updates their local model $\mat{W}^i_t$ (usually via SGD or some learning
algorithm) so that its update is given by $\mat{H}_t^i \triangleq \mat{W}_i^t -
\mat{W}_t$. The next global update is then given by 
\begin{equation}
    \mat{W}_{t + 1} = \mat{W}_t + \eta_t \mat{H}_t, \quad \mat{H}_t = \frac{1}{n_t}\sum_{i
    \in S_t} \mat{H}_t^i,
\end{equation} where $\eta_t$ is the learning rate.

\subsection{Structured Updates}%
\label{sec:structured_update}

\emph{Structured updates} send back and forth full-sized models based on a
predefined structure. By doing so, there is little loss of information over the
communication channel but potential for higher data throughput.

\subsubsection{Low Rank}%
\label{sub:low_rank}

We enforce each $\mat{H}_t^i \in \R^{d_1 \times d_2}$ to be a low rank matrix of
rank at most $k$ for a fixed $k$ by letting $\mat{H}^i_t = \mat{A}^i_t
\mat{B}^i_t$ where $\mat{A}_t^i \in \R^{d_1 \times k}$ and $\mat{B}_t^i \in
\R^{k \times d_2}$. This matrix is further compressed by representing $\mat{A}_t^i$
as a random matrix defined by a seed and only optimizing $\mat{B}_t^i$, saving a
factor of $d_1 / k$ parameters in communication. $\mat{A}_t^i$ is generated
afresh each round and for each client independently.

\subsubsection{Random Mask}%
\label{sub:random_mask}

We restrict each $\mat{H}_t^i$ to be sparse based on a predefined sparsity
pattern with respect to a random seed (similar to low rank). The sparsity
pattern is generated afresh each round and for each client independently.

\subsection{Sketched Updates}%
\label{sec:sketched_update}

\emph{Sketched updates} place higher emphasis on communication cost rather than
fully structured models.

\subsubsection{Subsampling}%
\label{sub:subsampling}

We only communicate a matrix $\hat{\mat{H}}_t^i = \mat{H}_t^i \odot \mat{M}$
where $\mat{M}$ is a sparse mask and is generated afresh each round and for each
client independently. This can be done s.t.\ $\E{\hat{\mat{H}}_t} = \mat{H}_t$.

\subsubsection{Probabilistic Quantization}%
\label{sub:probabilistic_quantization}

Let $\vec{h} = \left(h_1, \dots, h_{d_1 \times d_2}\right) =
\text{vec}\left(\mat{H}_t^i\right)$, $h_{\max} = \max_j h_j$, and $h_{\min} =
\min_j h_j$. We can apply $b$-bit quantization by dividing $[h_{\min}, h_{\max}]$
into $2^b$ intervals. Then if $h_i$ falls in the interval $[h_{\min}^i,
h_{\max}^i]$, then the compressed update for index $i$ is defined by 
\begin{equation}
    \tilde{h}_i = \begin{cases}
        h_{\max}^i, & \text{with probability } \frac{h_i -
        h_{\min}^i}{h_{\max}^i - h_{\min}^i} \\
        h_{\min}^i, & \text{with probability } \frac{h_{\max}^i -
        h_i}{h_{\max}^i - h_{\min}^i}.
    \end{cases}
\end{equation}
Using this, we have that $\E{\tilde{\vec{h}}} = \vec{h}$.

Error bounds on this method can be improved by a factor of $\cal{O}(d/\log d)$
by first performing a random rotation on $\vec{h}$, quantizing, sending the
information to the server, then performing the inverse rotation. The paper
states that a structured rotation matrix, the product of a Walsh-Hadamard and
binary diagonal entry matrix, can have a computational complexity of
$\cal{O}(d)$ for generating the matrix and $\cal{O}(d \log d)$ for applying the
matrix.


