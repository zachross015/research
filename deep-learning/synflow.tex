\chapter{\sectioncite{prune}}%
\label{cha:pruning_neural_networks_without_any_data_by_iteratively_conserving_synaptic_flow}

This paper, while theoretically captivating, is lackluster with providing
mathematical commentary on the concepts and proofs. It may be worthwhile to go
back and either notate the more mathematical definitions of the paper, or derive
my own definitions and reprove their claims using a more solid theoretical
foundation. This may even set a better groundwork for future 
analysis.

\section{Pruning Algorithms}%
\label{sub:pruning_algorithms}

Pruning algorithms are generally defined by 
\begin{enumerate}
    \item scoring parameters by some metric, and
    \item masking parameters according to their scores.
\end{enumerate} The latter is generally done by removing the parameters (e.g.
making the value zero via Hadamard product). This can either be done via global
masking or layer-masking, with global-masking performing better but suffering
from \emph{layer-collapse}, which is when an entire layer is masked.

Let $\theta$ be a collection of network parameters and $\thetaprune$ be the
parameters remaining after pruning. The \emph{compression ratio} $\rho$ of a
pruning algorithm is 
\begin{equation}\label{eq:crat}
    \rho = \frac{|\theta|}{|\thetaprune|}
\end{equation}. We define $\crm$ as the maximal possible compression
ratio for a \emph{network} that doesn't lead to layer collapse and the
$\crcc$ as the maximal compression ratio a given \emph{algorithm} can achieve without
inducing layer collapse. Note that the distinction in the two is $\crm$ is
maximal for a network while $\crcc$ is maximal for an algorithm. These
definitions motivate the following axiom:
\begin{axiom}[Maximal Critical Compression]
    For any pruning algorithm and any network, we should always have $\crcc =
    \crm$. 
\end{axiom}


\section{Synaptic Saliency}%
\label{sub:synaptic_saliency}

\emph{Synaptic saliency} is a class of score metrics defined by 
\begin{equation}
    \label{eq:synaptic_saliency}
    \cal{S}(\theta) = \pdv{\cal{R}}{\theta} \odot \theta
\end{equation} where $\cal{R}$ is a scalar loss function of the output $y$
of a feed-forward network parameterized by $\theta$. This metric satisfies two
conservation laws:
\begin{theorem}[Neuron-wise Conservation of Synaptic Saliency]
    For a feedforward neural network with continuous, homogeneous activation
    functions $\phi(x) = \phi'(x)x$, let $j$ be the index of a hidden neuron in
    layer $i$. The sum of the synaptic saliency for the
    incoming parameters to a hidden neuron $\left(\cal{S}^{(l)_{\text{in}}}_j = \synapsemetric{\theta_j^{(l)}} \right)$ is equal to the sum of the
    synaptic saliency for the outgoing parameters from the hidden neuron
    $\left(\cal{S}^{(l)_{\text{out}}}_j = \synapsemetric{\theta_{:,j}^{(l + 1)}}\right)$.
\end{theorem}
\begin{theorem}[Network-wise Conservation of Synaptic Saliency]
    The sum of the synaptic saliency across any set of parameters that exactly
    separates the input neurons $x$ from the output neurons $y$ of a feedforward
    neural network with homogeneous activation functions equals
    $\synapsemetric{x} = \synapsemetric{y}$.
\end{theorem}

\section{Algorithm}%
\label{sub:algorithm}


For the following theorem, we define the \emph{prune size} to be the total score
for the parameters pruned at any iteration and the \emph{cut size} to be the
total score for an entire layer.
\begin{theorem}
    If a pruning algorithm with global-masking assigns positive scores that
    respect layer-wise conservation, and if the prune size is strictly less than 
    the cut size whenever possible, then the algorithm satisfies the Maximal
    Critical Compression axiom. 
\end{theorem}

The pruning algorithm defined in this paper uses a loss function 
\begin{equation}
    \label{eq:lossr}
    \cal{R}_{\text{SF}} = \1^\intercal \left(\prod_{l = 1}^L
    \abs{\theta^{(l)}}\right) \1
\end{equation} so that for a fully connected network $\left(f_\theta(x) =
\theta^{(L)}\dots \theta^{(1)}\vec{x}\right)$ the Synaptic Flow score for a
parameter $\theta_{i, j}^{(l)}$ is 
\begin{equation}
    \cal{S}_{\text{SF}}(\theta_{i, j}^{(l)}) = \left[\1^\intercal
    \prod_{k = l + 1}^L \abs{\theta^{(k)}}\right]_i \abs{\theta_{i, j}^{(l)}}
    \left[\prod_{k = 1}^{l - 1} \abs{\theta^{(k)}}\1 \right]_j
\end{equation} i.e. the portion of the $l_1$-path norm the network has through
this parameter. This contributes to the development of \refalg{synflow}.


\begin{algorithm}[h]
    \caption{Iterative Synaptic Flow Pruning (SynFlow)}
    \label{alg:synflow}
    \begin{algorithmic}
        \Require network $f_\theta$, compression ratio $\rho$, iteration steps $n$
        \State $\mu = \1$ \Comment{Initialize binary mask}
        \For{$k$ in $[1,\dots, n]$}
            \State $\theta_\mu \leftarrow \mu \odot \theta_0$ \Comment{Mask parameters}
            \State $\cal{R} \leftarrow \1^\intercal \left(\prod_{l = 1}^L \abs{\theta^{(l)}_\mu}\right) \1$ \Comment{Evaluate SynFlow objective}
            \State $\cal{S} = \pdv{\cal{R}}{\theta_\mu} \odot \theta_\mu$ \Comment{Compute SynFlow score}
            \State $\tau \leftarrow \left(1 - \rho^{-k/n}\right)$ percentile of $\cal{S}$ \Comment{Find threshold}
            \State $\mu \leftarrow (\tau < \cal{S})$ \Comment{Update mask}
        \EndFor
        \State \Return $f_{\mu \odot \theta}$ \Comment{Return masked network}
    \end{algorithmic}
\end{algorithm}

\section{Commentary}%
\label{sub:commentary}

A thought I had: the loss function defined in \refeq{lossr} seems to almost
indicate a just-past-the-minimum metric for the predefined network. It
effectively is just the sum of the entries in the product of the network space.
Although I agree with the data agnostic approach, I feel there should be a more
suitable loss function which should seek to preserve this score rather than
minimize it, or at the very least, should compare this score against some
optimum, i.e.\ the optimum score for that network under those parameters. I also
feel that the prune size and cut size should have a greater impact over the
outcome of the resulting network, such that the compression ratio is fairly
equal for all layers.

\begin{research}
    Does SynFlow, or other pruning methods for that matter, produce the same
    network given different initial parameterizations? Furthermore, might we be
    able to use a genetic algorithm to find the ``lottery ticket''?
\end{research}
One thing that needs to be considered with this question is that it's entirely
possible that isomorphic networks could be produced, so there may need to be
some kind of check that occurs post pruning that confirms whether or not this
network has been discovered already.
