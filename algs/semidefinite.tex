\chapter{\sectioncite{semidefinite}}
\label{sec:semidefinite}

The initial problem attempted to be solved by this paper is creating an
approximation algorithm for \textsc{Max-Cut} to improve on the previous
guarantee of $\frac{3}{4}$ for an undirected graph $G = (V, E)$ where $|V| = n$
with nonnegative weights $W_{ij} = W_{ji}$ i.e. $\mat{W}$ denotes a weighted
adjacency matrix. 

We construct a cut of $G$ by assigning to each $i \in V$ a value $y_i \in \{-1,
1\}$ and construct $S \subset G$ by taking $S = \{i | y_i = 1\}$.
Define the value of a cut as 
\begin{equation}
    \label{eq:weightc}
    w(S, \bar{S}) = \frac{1}{2} \sum_{i < j} W_{ij} (1 - y_iy_j) 
\end{equation}
The \textsc{Max-Cut} integer quadratic program is then originally defined as 
\begin{equation}
    \label{eq:maxc}
    \textsc{Max Cut}(G) = \max_{S \subset V} w(S, \bar{S}), 
\end{equation} i.e.\ the goal is to find values $y_i$ for all $i \in V$ that
maximize this equation.

The proposed relaxation is to substitute each $y_i$ in \refeq{weightc} with some
vector $\vec{v}_i \in \R^m$ ($m \leq n$) such that $\norm{\vec{v}_i}^2 = 1$ so
that each $\vec{v}_i$ belongs to
the $m$-dimensional unit sphere $S_m$. Under this relaxation, we instead
construct $S \subset G$ by selecting some $\vec{r} \in S_m$ randomly and uniformly and
take $S = \{i | \dotp{\vec{v}_i, \vec{r}} \geq 0\}$ i.e. $\vec{r}$ determines a hyperplane which
separates vertices of $G$. Then \refeq{weightc} becomes
\begin{equation}
    \label{eq:weightp}
    w(S, \bar{S}) = \frac{1}{2} \sum_{i < j} W_{ij} (1 - \dotp{\vec{v}_i, \vec{v}_j}) .
\end{equation} 

\begin{remark}
    For a real, symmetric matrix $A$, the following are equivalent:
    \begin{itemize}
        \item A is positive semidefinite;
        \item $\lambda(A) \subset \R^+ \cup \{0\}$ where $\lambda(X)$ is the set
            of $X$'s eigenvalues; and
        \item $A = B^\intercal B$ for some $B \in \R^{ m \times n}$ where $m
            \leq n$.
    \end{itemize}
\end{remark}

The goal is now to determine optimal values for the unit vectors $\vec{v}_1, \dots,
\vec{v}_n$ and the dimensionality $m$ for the vector space they reside. We can take
$\mat{B}
= \bmat{\vec{v}_1 & \dots & \vec{v}_n}$ so that $\mat{Y} = \mat{B}^\intercal
\mat{B}$ is the Gram matrix of $\mat{B}$. Instead of solving for $\mat{B}$ right
away, we can instead note that each $Y_{ij} = \dotp{\vec{v}_i, \vec{v}_j}$ and
that our previous constraint is now just the constraint $Y_{ii} = 1$. We also
have that $Y$ is symmetric and PSD by construction. Semidefinite
programming can be used to maximize the equation 
\begin{equation}
    \frac{1}{2} \sum_{i < j}W_{ij}(1 - Y_{ij}).
\end{equation} Rather than deal with each $v_i$ directly, we can just use an
approximate semidefinite programming algorithm to find an optimal $Y$ in
$O\left(\sqrt{n}\left(\log\left(\sum_{i < j}W_{ij}\right) + \log 1 /
\epsilon\right)\right)$ iterations (Alizadeh's adaptation of Ye's interior-point
algorithm) with each iteration taking $O(n^3)$, where $\epsilon > 0$ denotes the
acceptable distance from the optimal value $Z_P^*$ (i.e.\ $Z_P^* - \epsilon$).
Once an optimal $\mat{Y}$ has been found, an incomplete Cholesky decomposition
can be used to obtain $\vec{v}_1, \dots, \vec{v}_n \in S_m$ s.t. 
\begin{equation}
    \frac{1}{2} \sum_{i < j} W_{ij} (1 - \dotp{\vec{v}_i, \vec{v}_j}) \geq
    Z_P^* - \epsilon.
\end{equation} To find $m$, they note that any \emph{extreme solution}, one
which cannot be expressed as the strict convex combination of other feasible
solutions, has at most rank $l$ where
\begin{equation}
    l \leq \frac{\sqrt{8n + 1} - 1}{2} < \sqrt{2n}.
\end{equation} Any other solution has $m \leq n$.

\section{Analysis}%
\label{sec:analysis}

The analysis breaks apart into three cases:
\begin{enumerate}
    \item the general case with nonnegative edge weights,
    \item an tighter bound when the weight of the cut constitutes a large
        portion of the overall weight,
    \item and extension to negative weights.
\end{enumerate}
The general case constitutes defining the overall expected weight as in 
\begin{equation}
    \E{W} = \frac{1}{\pi} \sum_{i < j}W_{ij}\pr{\sgn{\dotp{\vec{v}_i,
    \vec{r}}} \neq \sgn{\dotp{\vec{v}_j,
    \vec{r}}}}.
\end{equation} They then state the following lemma:
\begin{lemma}
    \begin{equation}
        \pr{\sgn{\dotp{\vec{v}_i,
        \vec{r}}} \neq \sgn{\dotp{\vec{v}_j,
        \vec{r}}}} = \frac{1}{\pi} \arccos(\dotp{\vec{v}_i,
        \vec{v}_j}),
    \end{equation}
\end{lemma} \noindent which follows from the fact that the probability that these two values
have the same sign is equivalent to the dihedral angle between the vectors,
i.e.\ the probability that a randomly selected hyperplane bisects the two
divided by all the possible hyperplanes that bisect the unit sphere. This
results in the theorem
\begin{theorem} 
    \begin{equation}
        \E{W} = \frac{1}{\pi} \sum_{i < j}W_{ij}\arccos(\dotp{\vec{v}_i,
        \vec{v}_j}).
    \end{equation} 
\end{theorem} 

Let $\theta = \arccos(\dotp{\vec{v}_i, \vec{v}_j})$ and define
\begin{equation}
    \label{eq:alpha-bound}
    \alpha = \min_{0 \leq \theta \leq \pi}\frac{2}{\pi}\frac{\theta}{1 -
    \cos\theta}.
\end{equation} They then provide the following lemmas
\begin{lemma}
    For $-1 \leq y \leq 1$, $\arccos(y) / \pi \geq \alpha \cdot \frac{1}{2}(1 - y)$,
\end{lemma} \noindent which follows from substituting $y = \cos \theta$, and 
\begin{lemma}
    $\alpha > 0.87856$
\end{lemma} \noindent which follows from finding a lower bound for
\refeq{alpha-bound}. They use these to show that 
\begin{theorem}
    \begin{equation}
        \E{W} \geq \alpha \frac{1}{2} \sum_{i < j} W_{ij} (1 - \dotp{\vec{v}_i,
        \vec{v}_j}).
    \end{equation}
\end{theorem}

For the case where the cut constitutes a large portion of the overall weight, we
define 
$W_{\text{tot}} = \sum_{i < j} W_{ij}$, $h(t) = {\arccos(1 -
2t)}/{\pi}$, and $\gamma = \argmin_{0 < t \leq 1}
{h(t)}/{t} \approx 0.84458$. The analysis then provides the theorem:
\begin{theorem}
    Let \begin{equation}
        A = \frac{1}{W_{\text{tot}}} \frac{1}{2}\sum_{i < j} W_{ij} (1 -
        \dotp{\vec{v}_i, \vec{v}_j}).
    \end{equation} If $A \geq \gamma$, then 
    \begin{equation}
        \E{W} \geq \frac{h(A)}{A}\frac{1}{2}\sum_{i < j} W_{ij} (1 -
        \dotp{\vec{v}_i, \vec{v}_j}),
    \end{equation}
\end{theorem} \noindent i.e.\ as $A$ varies between $\gamma$ and 1, $h(A) / A$ varies
between $\alpha$ and $1$. This provides us with a performance guarantee of
$h(A)/A - \epsilon$.
